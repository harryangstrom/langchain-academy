{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# LangChain Academy\n",
    "\n",
    "Welcome to LangChain Academy! \n",
    "\n",
    "# Importante\n",
    "\n",
    "Ejecutar jupyter lab desde el script .env, en el raiz del directorio langchain-academy, con objeto de propatar las variables de entorno que contienen las API_KEY.\n",
    "\n",
    "## Context\n",
    "\n",
    "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible. \n",
    "\n",
    "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state. \n",
    "\n",
    "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independently of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant. As noted, please ensure that you have an `OPENAI_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-stderr\n",
    "#%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7dfb303-3f74-403f-a939-fb428807fda6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qU\n",
      "  Downloading qu-1.2.5.tar.gz (3.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting qiniu>=7.2.0 (from qU)\n",
      "  Downloading qiniu-7.16.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click>=6.7 (from qU)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytest>=3.3.1 (from qU)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.52 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-google-genai) (0.3.53)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-google-genai) (2.11.3)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.3.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
      "Collecting iniconfig (from pytest>=3.3.1->qU)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=3.3.1->qU)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from qiniu>=7.2.0->qU) (2.32.3)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading grpcio-1.72.0rc1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading grpcio_status-1.72.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.14.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.3.1)\n",
      "Downloading langchain_google_genai-2.1.3-py3-none-any.whl (43 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading qiniu-7.16.0-py3-none-any.whl (80 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.72.0rc1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.72.0rc1-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: qU\n",
      "  Building wheel for qU (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for qU: filename=qu-1.2.5-py3-none-any.whl size=3879 sha256=34c8fb6b891f5d77340984a718fe0e415e9f2a1500d1b07b9dbd85057c424fa2\n",
      "  Stored in directory: /home/carlos/.cache/pip/wheels/fe/5a/e8/442185407f136961f7e28fdccec8370b7eaa936c6f16931ec4\n",
      "Successfully built qU\n",
      "Installing collected packages: filetype, pyasn1, protobuf, pluggy, iniconfig, grpcio, click, cachetools, rsa, qiniu, pytest, pyasn1-modules, proto-plus, googleapis-common-protos, qU, grpcio-status, google-auth, google-api-core, google-ai-generativelanguage, langchain-google-genai\n",
      "Successfully installed cachetools-5.5.2 click-8.1.8 filetype-1.2.0 google-ai-generativelanguage-0.6.17 google-api-core-2.24.2 google-auth-2.39.0 googleapis-common-protos-1.70.0 grpcio-1.72.0rc1 grpcio-status-1.72.0rc1 iniconfig-2.1.0 langchain-google-genai-2.1.3 pluggy-1.5.0 proto-plus-1.26.1 protobuf-6.30.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 pytest-8.3.5 qU-1.2.5 qiniu-7.16.0 rsa-4.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "#_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below. If you've run `pip install -r requirements.txt` as noted in the README, then you've installed the `langchain-openai` package. With this, we can instantiate our `ChatOpenAI` model object. If you are signing up for the API for the first time, you should receive [free credits](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) that can be applied to any of the models. You can see pricing for various models [here](https://openai.com/api/pricing/). The notebooks will default to `gpt-4o` because it's a good balance of quality, price, and speed [see more here](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), but you can also opt for the lower priced `gpt-3.5` series models. \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "#gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "#gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f0f838-924f-44d2-9d56-1d07f0990e09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qU in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (1.2.5)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: qiniu>=7.2.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from qU) (7.16.0)\n",
      "Requirement already satisfied: click>=6.7 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from qU) (8.1.8)\n",
      "Requirement already satisfied: pytest>=3.3.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from qU) (8.3.5)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-ollama) (0.3.53)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.3.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.11.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: iniconfig in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pytest>=3.3.1->qU) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pytest>=3.3.1->qU) (1.5.0)\n",
      "Requirement already satisfied: requests in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from qiniu>=7.2.0->qU) (2.32.3)\n",
      "Requirement already satisfied: anyio in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.8)\n",
      "Requirement already satisfied: idna in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from requests->qiniu>=7.2.0->qU) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/carlos/miniconda3/envs/lg-course/lib/python3.13/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Downloading langchain_ollama-0.3.2-py3-none-any.whl (20 kB)\n",
      "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.3.2 ollama-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7830cb64-e404-4b1b-894c-131123d418e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-pro-exp-03-25', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fb4d0-2d04-4ce0-9895-56e9e6996b45",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "Servidor ollama ejecutando en contenedor en raspberry pi 4b, con 8 GB de memoria.\n",
    "El procedimiento para su instalacion y configuración básica está [aquí](https://chat.deepseek.com/a/chat/s/ce244df9-07de-42ec-8dfb-bcecbbb5ce1f).\n",
    "A la redacción de esto, el servidor estaba publicando el servicio de ollama en http://192.168.1.126:11434.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d19c0c-e809-4e35-bce7-beb69067882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    #model=\"phi3:mini\",\n",
    "    model=\"tinyllama\",\n",
    "    temperature=0,\n",
    "    base_url = \"http://192.168.1.126:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am not capable of creating text or speech, but I can provide you with an example response to the question \"you are a helpful ai assistant.\"\\n\\nin response to your question, \"what is your role as an ai assistant?\" my response would be:\\n\\nas an ai assistant, I am designed to assist and support humans in their daily tasks. My primary function is to provide assistance with tasks such as answering questions, completing forms or documents, and performing basic tasks that require human input. However, I can also perform more complex tasks if necessary.\\n\\nmy role as an ai assistant is not limited to just providing assistance, but rather, I aim to enhance the overall user experience by providing a seamless and efficient solution for any task. By doing so, I aim to make your life easier and more convenient.', additional_kwargs={}, response_metadata={'model': 'tinyllama', 'created_at': '2025-04-18T10:09:54.639893889Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32954396922, 'load_duration': 52595974, 'prompt_eval_count': 36, 'prompt_eval_duration': 2803809165, 'eval_count': 171, 'eval_duration': 30094240602, 'model_name': 'tinyllama'}, id='run-f8412af4-c076-4e83-8821-556dcb63ebd3-0', usage_metadata={'input_tokens': 36, 'output_tokens': 171, 'total_tokens': 207})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Quantum computing, also known as \"quantum supremacy,\" is a type of computing that relies on the principles of quantum mechanics to perform calculations exponentially faster and more efficiently than classical computers. It involves manipulating quantum bits (qubits) using quantum mechanics, which can be used to solve complex problems that are currently beyond the reach of traditional computing methods.\\n\\nIn essence, quantum computing uses the properties of quantum mechanics to create a new type of computer architecture that is capable of performing calculations exponentially faster than classical computers. This means that quantum computers can perform tasks that would take days or even weeks on a classical computer, in seconds or minutes.\\n\\nOne of the most significant advantages of quantum computing is its potential for solving complex problems that are currently beyond the reach of traditional computing methods. For example, quantum computers could be used to solve problems related to chemistry, physics, and engineering, such as designing new materials with improved properties, developing new drugs, and optimizing manufacturing processes.\\n\\nAnother advantage of quantum computing is its potential for enabling new applications in fields like finance, healthcare, and security. For example, quantum computers could be used to solve complex financial problems related to risk management, portfolio optimization, and fraud detection. They could also be used to develop new algorithms for secure communication and encryption.\\n\\nIn summary, quantum computing is a groundbreaking technology that has the potential to revolutionize various fields of science and industry. It involves manipulating qubits using quantum mechanics, which can perform calculations exponentially faster than classical computers.', additional_kwargs={}, response_metadata={'model': 'tinyllama', 'created_at': '2025-04-18T10:11:42.065753846Z', 'done': True, 'done_reason': 'stop', 'total_duration': 61071400184, 'load_duration': 46727338, 'prompt_eval_count': 48, 'prompt_eval_duration': 1899218874, 'eval_count': 329, 'eval_duration': 59122222842, 'model_name': 'tinyllama'}, id='run-edb7e310-93f6-4c14-8a5c-23608784f794-0', usage_metadata={'input_tokens': 48, 'output_tokens': 329, 'total_tokens': 377})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"tell me what is quantum computing in less than twenty words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc2f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro-exp-03-25', 'safety_ratings': []}, id='run-c511fda2-a57a-41e4-838a-89655d720939-0', usage_metadata={'input_tokens': 3, 'output_tokens': 320, 'total_tokens': 323, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d69da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06f87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'content': 'To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of what’s possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future. [...] LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) – based AI agents.\\nIt views agent Objective Points about LangGraph and workflows as cyclic graph topologies, allowing for more variable and nuanced agent behaviors than linear execution models. [...] Frameworks such as LangGraph are becoming increasingly important as AI develops. LangGraph is making the next generation of AI applications possible by offering a versatile and strong framework for developing and overseeing AI agents.',\n",
       "  'score': 0.947386},\n",
       " {'title': 'What Is LangGraph and How to Use It? - DataCamp',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing agent runtimes. With LangGraph, we can easily build robust, scalable, and flexible multi-agent systems. [...] Home\\nTutorials\\nArtificial Intelligence\\n\\nLangGraph Tutorial: What Is LangGraph and How to Use It?\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\nContents\\nJun 26, 2024 \\xa0· 12 min read\\nContents\\n\\nWhat Is LangGraph?\\nGraph structure\\nState management\\n\\nCoordination\\n\\n\\nWhy LangGraph?\\n\\nSimplified development\\nFlexibility\\nScalability\\n\\nFault tolerance [...] If you want to learn more about the LangChain ecosystem, I recommend this introduction to LangChain.\\nWhat Is LangGraph?\\nLangGraph enables us to create stateful, multi-actor applications utilizing LLMs as easily as possible. It extends the capabilities of LangChain, introducing the ability to create and manage cyclical graphs, which are pivotal for developing sophisticated agent runtimes. The core concepts of LangGraph include: graph structure, state management, and coordination.\\nGraph structure',\n",
       "  'score': 0.93844646},\n",
       " {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
       "  'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph? [...] LangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts [...] Conclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!',\n",
       "  'score': 0.93468803}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
